{
 "cells": [
  {
   "source": [
    "## Char BiLSTM for MIT Movies\n",
    "I was going to make this repository a package with setup.py and everything but because of my deadlines and responsibilities at my current workplace I haven't got the time to do that so I shared the structure of the project in README.md file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from src.namedentityrecognizer.trainers import CharBilstmTrainer\n",
    "from src.namedentityrecognizer.models.char_lstm import CharBilstm\n",
    "from src.namedentityrecognizer.utils.processors import NerPreProcessor\n",
    "from src.namedentityrecognizer.data.build_dataset import CharCorpus, BuildData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/karaz/Desktop/NamedEntityRecognizer\n"
     ]
    }
   ],
   "source": [
    "# For finding the absolute path dynamically for every other user for the sake of this notebooks paths\n",
    "for path in globals()['_dh']:\n",
    "    if \"NamedEntityRecognizer\" in path.split(os.sep):\n",
    "        absolute_path = path\n",
    "        break\n",
    "print(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set: 9775 sentences\nTest set: 2443 sentences\n"
     ]
    }
   ],
   "source": [
    "dataset = CharCorpus(\n",
    "    input_folder=os.path.join(absolute_path, \"data/modified/mitmovies_tab_format\"),\n",
    "    min_word_freq=3,\n",
    "    batch_size=64,\n",
    ")\n",
    "print(f\"Train set: {len(dataset.train_dataset)} sentences\")\n",
    "print(f\"Test set: {len(dataset.test_dataset)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with tab as splitter for corpus of torch text to handle - Uncomment if needed -\n",
    "# Convert ->  O\tgood             -> to ->  good     O           \n",
    "# Convert ->  B-GENRE\tromantic -> to ->  romantic B-GENRE\n",
    "# Convert ->  I-GENRE\tcomedies -> to ->  comedies I-GENRE\n",
    "BuildData.create_finaldata(os.path.join(absolute_path, \"data/raw/mitmovies/engtrain.bio\"), os.path.join(absolute_path, \"data/modified/mitmovies_tab_format/train.txt\"), splits=\"\\t\")\n",
    "BuildData.create_finaldata(os.path.join(absolute_path, \"data/raw/mitmovies/engtest.bio\"), os.path.join(absolute_path, \"data/modified/mitmovies_tab_format/test.txt\"), splits=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 1,028,749 trainable parameters.\nCharBilstm(\n  (embedding): Embedding(2244, 300, padding_idx=1)\n  (emb_dropout): Dropout(p=0.5, inplace=False)\n  (char_emb): Embedding(39, 25, padding_idx=1)\n  (char_cnn): Conv1d(25, 125, kernel_size=(3,), stride=(1,), groups=25)\n  (cnn_dropout): Dropout(p=0.25, inplace=False)\n  (lstm): LSTM(425, 64, num_layers=2, dropout=0.1, bidirectional=True)\n  (fc_dropout): Dropout(p=0.25, inplace=False)\n  (fc): Linear(in_features=128, out_features=26, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "bilstm = CharBilstm(\n",
    "    input_dim=len(dataset.word_field.vocab),\n",
    "    embedding_dim=300,\n",
    "    char_emb_dim=25,\n",
    "    char_input_dim=len(dataset.char_field.vocab),\n",
    "    char_cnn_filter_num=5,\n",
    "    char_cnn_kernel_size=3,\n",
    "    hidden_dim=64,\n",
    "    output_dim=len(dataset.tag_field.vocab),\n",
    "    lstm_layers=2,\n",
    "    emb_dropout=0.5,\n",
    "    cnn_dropout=0.25,\n",
    "    lstm_dropout=0.1,\n",
    "    fc_dropout=0.25,\n",
    "    word_pad_idx=dataset.word_pad_idx,\n",
    "    char_pad_idx=dataset.char_pad_idx\n",
    ")\n",
    "bilstm.init_embeddings(\n",
    "    char_pad_idx=dataset.char_pad_idx,\n",
    "    word_pad_idx=dataset.word_pad_idx,\n",
    "    pretrained=None,\n",
    "    freeze=True\n",
    ")\n",
    "print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n",
    "print(bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-67880e98021a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mlog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"char_bilstm_vanilla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/NamedEntityRecognizer/src/namedentityrecognizer/trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NamedEntityRecognizer/src/namedentityrecognizer/trainers.py\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy/Train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_global\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ner = CharBilstmTrainer(\n",
    "  model=bilstm,\n",
    "  data=dataset,\n",
    "  optimizer_cls=Adam,\n",
    "  loss_fn_cls=nn.CrossEntropyLoss,\n",
    "  log_file=\"char_bilstm_vanilla\"\n",
    ")\n",
    "ner.train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word    \tunk     \tpred tag\n4       \t        \tO      \nstar    \t        \tO      \nmovies  \t        \tO      \nthat    \t        \tO      \nNicholas\t        \tB-ACTOR\nCage    \t        \tI-ACTOR\nis      \t        \tO      \nplaying \t        \tO      \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['4', 'star', 'movies', 'that', 'Nicholas', 'Cage', 'is', 'playing'],\n",
       " ['O', 'O', 'O', 'O', 'B-ACTOR', 'I-ACTOR', 'O', 'O'],\n",
       " [])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ner.infer(\"4 star movies that Nicholas Cage is playing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('project': conda)",
   "metadata": {
    "interpreter": {
     "hash": "14dbe4d4df743aee721c7bd8dbc75325a1aee7c29f250bc6dc30066efadd3ddb"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}