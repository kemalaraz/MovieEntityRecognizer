{
 "cells": [
  {
   "source": [
    "## BiLSTM for MIT Movies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from src.namedentityrecognizer.models.lstm import BiLSTM\n",
    "from src.namedentityrecognizer.trainers import TrainerBiLstm\n",
    "from src.namedentityrecognizer.utils.processors import NerPreProcessor\n",
    "from src.namedentityrecognizer.data.build_dataset import Corpus, BuildData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/karaz/Desktop/NamedEntityRecognizer\n"
     ]
    }
   ],
   "source": [
    "# For finding the absolute path dynamically for every other user for the sake of this notebooks paths\n",
    "for path in globals()['_dh']:\n",
    "    if \"NamedEntityRecognizer\" in path.split(os.sep):\n",
    "        absolute_path = path\n",
    "        break\n",
    "print(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with tab as splitter for corpus of torch text to handle - Uncomment if needed -\n",
    "# Convert ->  O\tgood             -> to ->  good     O           \n",
    "# Convert ->  B-GENRE\tromantic -> to ->  romantic B-GENRE\n",
    "# Convert ->  I-GENRE\tcomedies -> to ->  comedies I-GENRE\n",
    "BuildData.create_finaldata(os.path.join(absolute_path, \"data/raw/mitmovies/engtrain.bio\"), os.path.join(absolute_path, \"data/modified/mitmovies_tab_format/train.txt\"), splits=\"\\t\")\n",
    "BuildData.create_finaldata(os.path.join(absolute_path, \"data/raw/mitmovies/engtest.bio\"), os.path.join(absolute_path, \"data/modified/mitmovies_tab_format/test.txt\"), splits=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set: 9775 sentences\nTest set: 2443 sentences\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "dataset = Corpus(\n",
    "    input_folder=os.path.join(absolute_path, \"data/modified/mitmovies_tab_format\"),\n",
    "    min_word_freq=3,  # any words occurring less than 3 times will be ignored from vocab\n",
    "    batch_size=64)\n",
    "print(f\"Train set: {len(dataset.train_dataset)} sentences\")\n",
    "print(f\"Test set: {len(dataset.test_dataset)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 1,161,930 trainable parameters.\nBiLSTM(\n  (embedding): Embedding(2244, 300, padding_idx=1)\n  (emb_dropout): Dropout(p=0.25, inplace=False)\n  (lstm): LSTM(300, 64, num_layers=4, dropout=0.01, bidirectional=True)\n  (fc_dropout): Dropout(p=0.1, inplace=False)\n  (fc): Linear(in_features=128, out_features=26, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM(\n",
    "    input_dim=len(dataset.word_field.vocab),\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=64,\n",
    "    output_dim=len(dataset.tag_field.vocab),\n",
    "    lstm_layers=4,\n",
    "    emb_dropout=0.25,\n",
    "    lstm_dropout=0.01,\n",
    "    fc_dropout=0.1,\n",
    "    word_pad_idx=dataset.word_pad_idx,\n",
    ")\n",
    "# Initialize weights and embeddings\n",
    "model.init_weights()\n",
    "model.init_embeddings(word_pad_idx=dataset.word_pad_idx)\n",
    "print(f\"The model has {model.count_parameters():,} trainable parameters.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.021 | Trn Acc: 0.01%\n",
      "\tVal Loss: 3.176 | Val Acc: 1.51% | Val Precision: 1.45% | Val Recall: 100.00% | Val F1 Macro: 2.84% | Val F1 Micro: 2.84%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.021 | Trn Acc: 0.02%\n",
      "\tVal Loss: 3.098 | Val Acc: 15.30% | Val Precision: 19.15% | Val Recall: 71.91% | Val F1 Macro: 14.04% | Val F1 Micro: 18.24%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.020 | Trn Acc: 0.27%\n",
      "\tVal Loss: 3.011 | Val Acc: 51.17% | Val Precision: 30.76% | Val Recall: 56.21% | Val F1 Macro: 35.95% | Val F1 Micro: 61.97%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.019 | Trn Acc: 0.40%\n",
      "\tVal Loss: 2.910 | Val Acc: 58.61% | Val Precision: 41.00% | Val Recall: 70.42% | Val F1 Macro: 51.05% | Val F1 Micro: 71.43%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.018 | Trn Acc: 0.42%\n",
      "\tVal Loss: 2.792 | Val Acc: 60.14% | Val Precision: 57.97% | Val Recall: 99.97% | Val F1 Macro: 72.94% | Val F1 Micro: 73.55%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.018 | Trn Acc: 0.38%\n",
      "\tVal Loss: 2.654 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.016 | Trn Acc: 0.41%\n",
      "\tVal Loss: 2.498 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.015 | Trn Acc: 0.39%\n",
      "\tVal Loss: 2.333 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.014 | Trn Acc: 0.41%\n",
      "\tVal Loss: 2.178 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.40%\n",
      "\tVal Loss: 2.058 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 11 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.40%\n",
      "\tVal Loss: 1.986 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 12 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.39%\n",
      "\tVal Loss: 1.947 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 13 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.39%\n",
      "\tVal Loss: 1.920 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 14 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.41%\n",
      "\tVal Loss: 1.895 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 15 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.014 | Trn Acc: 0.38%\n",
      "\tVal Loss: 1.873 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 16 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.014 | Trn Acc: 0.37%\n",
      "\tVal Loss: 1.856 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 17 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.013 | Trn Acc: 0.39%\n",
      "\tVal Loss: 1.847 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 18 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.012 | Trn Acc: 0.42%\n",
      "\tVal Loss: 1.845 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 19 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.012 | Trn Acc: 0.41%\n",
      "\tVal Loss: 1.848 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n",
      "Epoch: 20 | Epoch Time: 0m 0s\n",
      "\tTrn Loss: 0.012 | Trn Acc: 0.41%\n",
      "\tVal Loss: 1.851 | Val Acc: 60.17% | Val Precision: 58.39% | Val Recall: 100.00% | Val F1 Macro: 73.58% | Val F1 Micro: 73.58%\n"
     ]
    }
   ],
   "source": [
    "ner = TrainerBiLstm(\n",
    "  model=model,\n",
    "  data=dataset,\n",
    "  optimizer_cls=Adam,\n",
    "  loss_fn_cls=nn.CrossEntropyLoss,\n",
    "  log_name=\"bilstm_vanilla2\"\n",
    ")\n",
    "ner.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word \tunk  \tpred tag\nmovie\t     \tO   \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['list', 'an', 'r', 'rated', 'drama', 'movie'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O'],\n",
       " [])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ner.infer(\"list an r rated drama movie\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('project': conda)",
   "metadata": {
    "interpreter": {
     "hash": "14dbe4d4df743aee721c7bd8dbc75325a1aee7c29f250bc6dc30066efadd3ddb"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}